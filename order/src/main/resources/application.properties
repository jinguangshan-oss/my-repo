#端口
server.port=9001
#环境
spring.profiles.active=dev

#定义静态资源路径
spring.web.resources.static-locations=classpath:/META-INF/resources/,classpath:/resources/,classpath:/static/,classpath:/public/,classpath:/templates/

#注册中心
spring.cloud.nacos.discovery.service=order
spring.cloud.nacos.discovery.server-addr=192.168.1.6:8848
#配置中心
spring.cloud.nacos.config.namespace=
spring.cloud.nacos.config.group=DEFAULT_GROUP
spring.cloud.nacos.config.name=order
spring.cloud.nacos.config.server-addr=192.168.1.6:8848
spring.cloud.nacos.config.file-extension=properties
spring.config.import=nacos:${spring.cloud.nacos.config.name}-${spring.profiles.active}.${spring.cloud.nacos.config.file-extension}

#mybatis
mybatis.mapper-locations=classpath:/mappers/*.xml

#数据源
spring.datasource.url=jdbc:mysql://192.168.1.6:3306/order?characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true
spring.datasource.username=root
spring.datasource.password=root

#redis
spring.redis.host=192.168.1.6
spring.redis.port=6379

#rabbitmq
spring.rabbitmq.addresses=192.168.1.6
spring.rabbitmq.port=5672
spring.rabbitmq.username=admin
spring.rabbitmq.password=admin
#开启发布确认机制
spring.rabbitmq.publisher-confirm-type=correlated
#开启消费手动确认机制
spring.rabbitmq.listener.simple.acknowledge-mode=manual

#zookeeper
zookeeper.host=192.168.1.6

#kafka
spring.kafka.bootstrap-servers=192.168.1.6:9092,192.168.1.6:9093,192.168.1.6:9094
#生产者消息重发次数
spring.kafka.producer.retries=0
#生产者序列化器
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
#消费者组(同一个组内的多个消费者，消费同一个topic，则每条消息只能被一个消费者消费)
spring.kafka.consumer.group-id=myorder
#消费者反序列化器
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
#消费者每次拉取返回的最大记录数
spring.kafka.consumer.max-poll-records=10
#消费者监听的topic不存在时，保证能够是项目启动
spring.kafka.listener.missing-topics-fatal=false

#kafka-streams
spring.kafka.streams.application-id=order
# 会覆盖 spring.kafka.bootstrap-servers 配置
spring.kafka.streams.bootstrap-servers=192.168.1.6:9092,192.168.1.6:9093,192.168.1.6:9094
# 序列化器
spring.kafka.streams.properties.default.key.serde=org.apache.kafka.common.serialization.Serdes$StringSerde
spring.kafka.streams.properties.default.value.serde=org.apache.kafka.common.serialization.Serdes$StringSerde
spring.kafka.streams.properties.default.timestamp.extractor=org.apache.kafka.streams.processor.WallclockTimestampExtractor
# 允许json反序列化的包
spring.kafka.streams.properties.default.spring.json.trusted.packages=com.engrz.lab.*


#xxl-job
### xxl-job admin address list, such as "http://address" or "http://address01,http://address02"
xxl.job.admin.addresses=http://192.168.1.6:8077/xxl-job-admin
### xxl-job, access token
xxl.job.accessToken=default_token
### xxl-job executor appname
xxl.job.executor.appname=goods
### xxl-job executor registry-address: default use address to registry , otherwise use ip:port if address is null
xxl.job.executor.address=
### xxl-job executor server-info
xxl.job.executor.ip=192.168.1.11
xxl.job.executor.port=9997
### xxl-job executor log-path
xxl.job.executor.logpath=/usr/local/xxl-job/logs/jobhandler
### xxl-job executor log-retention-days
xxl.job.executor.logretentiondays=3